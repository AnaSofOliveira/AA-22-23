{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import mglearn\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho corpus: 50000 documentos\n",
      "Distribuição de classes:  [25000 25000]\n"
     ]
    }
   ],
   "source": [
    "fN = '/Users/anaso/Documents/AA/AA-22-23/Trabalho Final/imdbFull.p'\n",
    "\n",
    "D = pickle.load(open(fN, 'rb'))\n",
    "\n",
    "corpus = D.data\n",
    "y = D.target\n",
    "y = [1 if val>5 else 0 for val in y]\n",
    "\n",
    "print(\"Tamanho corpus: {} documentos\".format(len(corpus)))\n",
    "print(\"Distribuição de classes: \", np.bincount(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, y_train, y_test = train_test_split(corpus, y, test_size=0.5, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho corpus: 25000 documentos\n",
      "Documento Nro 4: \n",
      " Having grown up in the typing pool era and dealing with office politics and men who were apt to make a banquet of beauties into a delightful meal day to day, this movie hits the mark. Good afternoon fare. I understand that Louis Jordan wanted to work in this movie to play opposite the quintessential GORGEOUS Suzy Parker. Everyone wanted to be in and I believe it has done well and held up over time. Best on screen kiss between Hope Lange and the late Stephen Boyd.<br /><br />It may not appear that anything of this is plausible but actually it was and probably still is even given the scare of sexual harassment. I thought the movie was well cast except the awful acting of Evans. What a grease ball but he found his niche someplace else. Other than that, all stepped up to the plate.\n"
     ]
    }
   ],
   "source": [
    "num_doc = 3\n",
    "\n",
    "print(\"Tamanho corpus: {} documentos\".format(len(text_train)))\n",
    "print(\"Documento Nro {}: \\n {}\".format(num_doc+1, text_train[num_doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpeza de mudanças de linha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1 = [critica.replace(\"<br />\", \" \") for critica in text_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho corpus: 25000 documentos\n",
      "Documento Nro 4: \n",
      " Having grown up in the typing pool era and dealing with office politics and men who were apt to make a banquet of beauties into a delightful meal day to day, this movie hits the mark. Good afternoon fare. I understand that Louis Jordan wanted to work in this movie to play opposite the quintessential GORGEOUS Suzy Parker. Everyone wanted to be in and I believe it has done well and held up over time. Best on screen kiss between Hope Lange and the late Stephen Boyd.  It may not appear that anything of this is plausible but actually it was and probably still is even given the scare of sexual harassment. I thought the movie was well cast except the awful acting of Evans. What a grease ball but he found his niche someplace else. Other than that, all stepped up to the plate. | Palavras: 145\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamanho corpus: {} documentos\".format(len(corpus1)))\n",
    "print(\"Documento Nro {}: \\n {} | Palavras: {}\".format(num_doc+1, corpus1[num_doc], len(corpus1[num_doc].split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpeza alfanumérica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho corpus: 25000 documentos\n"
     ]
    }
   ],
   "source": [
    "corpus_alfanum = [re.sub(r'[^A-Za-z0-9]+', ' ', critica) for critica in corpus1]\n",
    "print(\"Tamanho corpus: {} documentos\".format(len(corpus_alfanum)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpeza alfabética"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho corpus: 25000 documentos\n"
     ]
    }
   ],
   "source": [
    "corpus_alfab = [re.sub(r'[^a-zA-Z]+', ' ', critica) for critica in corpus1]\n",
    "print(\"Tamanho corpus: {} documentos\".format(len(corpus_alfab)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparação limpeza alfanumérica e limpeza alfabética"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sem limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpeza Alfanumérica: \n",
      "Tamanho do dicionário: 76739\n",
      "Mean cross-validation accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer().fit(corpus1)\n",
    "X_train = tf.transform(corpus1)\n",
    "\n",
    "dicionario_sem_limpeza = len(tf.get_feature_names_out())\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Limpeza Alfanumérica: \")\n",
    "print(\"Tamanho do dicionário: {}\".format(len(tf.get_feature_names_out())))\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Limpeza Alfanumérica: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpeza Alfanumérica: \n",
      "Tamanho do dicionário: 76397\n",
      "Mean cross-validation accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer().fit(corpus_alfanum)\n",
    "X_train = tf.transform(corpus_alfanum)\n",
    "\n",
    "dicionario_limpeza_alfanumerica = len(tf.get_feature_names_out())\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Limpeza Alfanumérica: \")\n",
    "print(\"Tamanho do dicionário: {}\".format(len(tf.get_feature_names_out())))\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Limpeza Alfabética"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Limpeza Alfabética: \n",
      "Tamanho do dicionário: 75193\n",
      "Mean cross-validation accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer().fit(corpus_alfab)\n",
    "X_train = tf.transform(corpus_alfab)\n",
    "\n",
    "dicionario_limpeza_alfabetica = len(tf.get_feature_names_out())\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_train, y_train, cv=5)\n",
    "\n",
    "print(\"\\nLimpeza Alfabética: \")\n",
    "print(\"Tamanho do dicionário: {}\".format(len(tf.get_feature_names_out())))\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NOTA:</b> \n",
    "- Como a limpeza alfabética minimiza o tamanho do dicionário mantendo a precisão do classificador, será esta a limpeza/filtragem que irei usar no produto final. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anaso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "\n",
    "no_stop_words = [\" \".join([(word if word not in (stop_words) else \"\") for word in critica.split()]) for critica in corpus_alfab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Limpeza Alfabética: \n",
      "Tamanho do dicionário: 75188\n",
      "Mean cross-validation accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer().fit(no_stop_words)\n",
    "X_train = tf.transform(no_stop_words)\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_train, y_train, cv=5)\n",
    "\n",
    "print(\"\\nLimpeza Alfabética: \")\n",
    "print(\"Tamanho do dicionário: {}\".format(len(tf.get_feature_names_out())))\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NOTAS: </b> \n",
    "- As stop words correspondem às palavras com maior frequência de utilização no alfabeto inglês, pelo que estas palavras não serão as melhores para discriminar as avalições das criticas com base na informação textual da crítica. \n",
    "- Tal como anteriormente, existe uma diminuição no tamanho do vocabulário sem que a precisão do classificador fique comprometida, pelo que irei remover as stop words das criticas recolhidas no dataset IMDB. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização Vs Lemmatização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(documents, stemmer_name=\"porter\"):\n",
    "    stem = {\"porter\": PorterStemmer(), \n",
    "            \"lancaster\": LancasterStemmer(), \n",
    "            \"snowball\": SnowballStemmer(\"english\"), \n",
    "            \"lemma\": WordNetLemmatizer()} \n",
    "\n",
    "    stemmer = stem.get(stemmer_name) \n",
    "\n",
    "    if(stemmer_name==\"lemma\"):\n",
    "        return [\" \".join([stemmer.lemmatize(word, \"v\") for word in critic.split()]) for critic in documents]\n",
    "    else:  \n",
    "        return [\" \".join([stemmer.stem(word) for word in critic.split()]) for critic in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_porter = tokenization(no_stop_words, stemmer_name=\"porter\")\n",
    "\n",
    "tf_porter = TfidfVectorizer().fit(docs_porter)\n",
    "X_train_porter = tf_porter.transform(docs_porter)\n",
    "\n",
    "dicionario_porter = len(tf_porter.get_feature_names_out())\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_train_porter, y_train, cv=5)\n",
    "\n",
    "print(\"\\nLimpeza Alfabética: \")\n",
    "print(\"Tamanho do dicionário: {}\".format(len(tf_porter.get_feature_names_out())))\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_snow = tokenization(no_stop_words, stemmer_name=\"snowball\")  \n",
    "\n",
    "tf_snowball = TfidfVectorizer().fit(docs_snow)\n",
    "X_train_snoball = tf_snowball.transform(docs_snow)\n",
    "\n",
    "dicionario_snowball = len(tf_snowball.get_feature_names_out())\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_train_snoball, y_train, cv=5)\n",
    "\n",
    "print(\"\\nLimpeza Alfabética: \")\n",
    "print(\"Tamanho do dicionário: {}\".format(len(tf_snowball.get_feature_names_out())))\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_lanc = tokenization(no_stop_words, stemmer_name=\"lancaster\")\n",
    "\n",
    "tf_lancaster = TfidfVectorizer().fit(docs_lanc)\n",
    "X_train_lancaster = tf_lancaster.transform(docs_lanc)\n",
    "\n",
    "dicionario_lancaster = len(tf_lancaster.get_feature_names_out())\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_train_lancaster, y_train, cv=5)\n",
    "\n",
    "print(\"\\nLimpeza Alfabética: \")\n",
    "print(\"Tamanho do dicionário: {}\".format(len(tf_lancaster.get_feature_names_out())))\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lematização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_lemma = tokenization(no_stop_words, stemmer_name=\"lemma\")  \n",
    "\n",
    "tf_lemma = TfidfVectorizer().fit(docs_lemma)\n",
    "X_train_lemma = tf_lemma.transform(docs_lemma)\n",
    "\n",
    "dicionario_lemma = len(tf_lemma.get_feature_names_out())\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_train_lemma, y_train, cv=5)\n",
    "\n",
    "print(\"\\nLimpeza Alfabética: \")\n",
    "print(\"Tamanho do dicionário: {}\".format(len(tf_lemma.get_feature_names_out())))\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionarios = ['Sem Limpeza', 'Limpeza Alfanumérica', 'Limpeza Alfabética']\n",
    "tamanhos = [dicionario_sem_limpeza, dicionario_limpeza_alfanumerica, dicionario_limpeza_alfabetica]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(dicionarios, tamanhos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionarios = ['Sem stemming/lemmatização', 'Porter Stemmer', 'Snowball Stemmer', 'Lancaster Stemmer', 'Lemmatização']\n",
    "tamanhos = [dicionario_limpeza_alfabetica, dicinario_porter, dicionario_snowball, dicionario_lancaster, dicionario_lemma]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(dicionarios, tamanhos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=10000)\n",
    "\n",
    "y_train_porter = lr.fit(X_train_porter, y_train)\n",
    "fpr_porter, tpr_porter, thresholds_porter = roc_curve(y_train, lr.predict_proba(X_train_porter)[:, 1])\n",
    "\n",
    "y_train_snow = lr.fit(X_train_snoball, y_train)\n",
    "fpr_snow, tpr_snow, thresholds_snow = roc_curve(y_train, lr.predict_proba(X_train_snoball)[:, 1])\n",
    "\n",
    "y_train_lanc = lr.fit(X_train_lancaster, y_train)\n",
    "fpr_lanc, tpr_lanc, thresholds_lanc = roc_curve(y_train, lr.predict_proba(X_train_lancaster)[:, 1])\n",
    "\n",
    "y_train_lemma = lr.fit(X_train_lemma, y_train)\n",
    "fpr_lemma, tpr_lemma, thresholds_lemma = roc_curve(y_train, lr.predict_proba(X_train_lemma)[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"ROC Curve \\n Tokenização Vs Lemmatização\")\n",
    "plt.plot(fpr_porter, tpr_porter, label=\"Porter\")\n",
    "plt.plot(fpr_lanc, tpr_lanc, label=\"Lancaster\")\n",
    "plt.plot(fpr_snow, tpr_snow, label=\"Snowball\")\n",
    "plt.plot(fpr_lemma, tpr_lemma, label=\"Lemma\")\n",
    "\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "plt.axis(\"scaled\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NOTAS:</b>\n",
    "- A tokenização reduz palavras à sua raiz comum. No entanto, este processo não tem em consideração a composição natural das palavras. Por exemplo, palavras como \"car\" e \"care\" são ambas convertidas para \"car\", eliminando o contexto das mesmas. \n",
    "- A lemmatização é um processo identifico à tokenização, porém considera a composição natural das palavras. Assim, e conforme o exemplo acima as palavras \"car\" e \"care\" são mantidas na sua forma canónica, nunca perdendo o seu contexto.\n",
    "- Devido aos fatores explicados acima, e após a extração de caracteristicas, a tokenização minimiza o tamanho do dicionário, enquanto que a lematização produz mais tokens. Ainda assim, qualquer um destes processos, diminui o tamanho original de tokens. \n",
    "- Tendo em consideração que pretendo classificar as criticas no dataset do IMDB com base na sua informação textual, o significado das palavras é bastante importante. Por isso, e que com este pré-processamento do texto melhora os resultados do classificador, irei utilizar a lematização para a análise pretendida.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b40f68814b93475b29075cb0adeb1712a4387a9e06914676246b503f1ae483b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
